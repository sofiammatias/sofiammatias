{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sofiammatias/sofiammatias/blob/main/mnist_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eBH-F3D7cN_"
      },
      "source": [
        "# MNIST Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s-AW0wD7cOF"
      },
      "source": [
        "üéØ <b><u>Exercise objectives</u></b>\n",
        "- Understand the *MNIST* dataset \n",
        "- Design your first **Convolutional Neural Network** (*CNN*) and answer questions such as:\n",
        "    - what are *Convolutional Layers*? \n",
        "    - how many *parameters* are involved in such a layer?\n",
        "- Train this CNN on images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdE0I4oa7cOa"
      },
      "source": [
        "üöÄ <b><u>Let's get started!</u></b>\n",
        "\n",
        "Imagine that we are  back in time into the 90's.\n",
        "You work at a *Post Office* and you have to deal with an enormous amount of letters on a daily basis. How could you automate the process of reading the ZIP Codes, which are a combination of 5 handwritten digits? \n",
        "\n",
        "This task, called the **Handwriting Recognition**, used to be a very complex problem back in those days. It was solved by *Bell Labs* (among others) where one of the Deep Learning gurus, [*Yann Le Cun*](https://en.wikipedia.org/wiki/Yann_LeCun), used to work.\n",
        "\n",
        "From [Wikipedia](https://en.wikipedia.org/wiki/Handwriting_recognition):\n",
        "\n",
        "> Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFtsNoe-7cOb"
      },
      "source": [
        "![Number recognition](recognition.gif)\n",
        "\n",
        "*Note: The animation above is just here to help you visualize what happens with the different images: <br/> $\\rightarrow$ For each image, once the CNN is trained, it will predict what digit is written. The inputs are the different digits and not one animation/video!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJxbd-oh7cOd"
      },
      "source": [
        "ü§î <b><u>How does this CNN work ?</u></b>\n",
        "\n",
        "- *Inputs*: Images (_each image shows a handwritten digit_)\n",
        "- *Target*: For each image, you want your CNN model to predict the correct digit (between 0 and 9)\n",
        "    - It is a **multi-class classification** task (more precisely a 10-class classification task since there are 10 different digits).\n",
        "\n",
        "üî¢ To improve the capacity of the Convolutional Neural Network to read these numbers, we need to feed it with many images representing handwritten digits. This is why the üìö [**MNIST dataset**](http://yann.lecun.com/exdb/mnist/) *(Mixed National Institute of Standards and Technology)* was created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CfKjBK507cOe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL1m7sQ-7cOg"
      },
      "source": [
        "## (1) The `MNIST` Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4hnMs_q7cOg"
      },
      "source": [
        "üìö Tensorflow/Keras offers multiple [**datasets**](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) to play with:\n",
        "- *Vectors*: `boston_housing` (regression)\n",
        "- *Images* : `mnist`, `fashion_mnist`, `cifar10`, `cifar100` (classification)\n",
        "- *Texts*: `imbd`, `reuters` (classification/sentiment analysis)\n",
        "\n",
        "\n",
        "üíæ You can **load the MNIST dataset** with the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9PkuTBZ7cOh",
        "outputId": "4fa51d98-a675-4999-bbb6-8284315e81e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(((60000, 28, 28), (60000,)), ((10000, 28, 28), (10000,)))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from tensorflow.keras import datasets\n",
        "\n",
        "\n",
        "# Loading the MNIST Dataset...\n",
        "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")\n",
        "\n",
        "# The train set contains 60 000 images, each of them of size 28x28\n",
        "# The test set contains 10 000 images, each of them of size 28x28\n",
        "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HiQ9Qrv7cOi"
      },
      "source": [
        "### (1.1) Exploring the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw8tLbMB7cOj"
      },
      "source": [
        "‚ùì **Question: Let's have look at some handwritten digits of this MNIST dataset.** ‚ùì\n",
        "\n",
        "üñ® Print some images from the *train set*.\n",
        "\n",
        "<details>\n",
        "    <summary><i>Hints</i></summary>\n",
        "\n",
        "üí°*Hint*: use the `imshow` function from `matplotlib` with `cmap = \"gray\"`\n",
        "\n",
        "ü§® Note: if you don't specify this *cmap* argument, the weirdly displayed colors are just Matplotlib defaults...\n",
        "    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "tags": [
          "challengify"
        ],
        "id": "5rdxKqTY7cOk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "20a46bf4-ecc5-4217-9cca-9b09829052f8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x864 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig, ax = plt.subplots(2,2, figsize=(12,12))\n",
        "ax[0,0].imshow(X_train[0], cmap='gray');\n",
        "ax[0,1].imshow(X_train[35], cmap='gray');\n",
        "ax[1,0].imshow(X_train[3567], cmap='gray');\n",
        "ax[1,1].imshow(X_train[20000], cmap='gray');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQQ0fd2d7cOk"
      },
      "source": [
        "### (1.2) Image Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRa5KIOw7cOl"
      },
      "source": [
        "‚ùóÔ∏è **Neural Networks converge faster when the input data is somehow normalized** ‚ùóÔ∏è\n",
        "\n",
        "üë©üèª‚Äçüè´ How do we proceed for Convolutional Neural Networks ?\n",
        "* The `RBG` intensities are coded between 0 and 255. \n",
        "* We can simply divide the input data by the maximal value 255 to have all the pixels' intensities between 0 and 1 üòâ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i10QI9ad7cOm"
      },
      "source": [
        "‚ùì **Question ‚ùì As a first preprocessing step, please normalize your data.** \n",
        "\n",
        "Don't forget to do it both on your train data and your test data.\n",
        "\n",
        "(*Note: you can also center your data, by subtracting 0.5 from all the values, but it is not mandatory*)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize\n",
        "X_train = X_train/255\n",
        "X_test = X_test/255\n",
        "y_train = y_train/255\n",
        "y_test = y_test/255"
      ],
      "metadata": {
        "id": "7NY_otXiP9Ei"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyicKvYI7cOn"
      },
      "source": [
        "### (1.3) Inputs' dimensionality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X9yVOk37cOo"
      },
      "source": [
        "üëÜ Remember that you have 60,000 training images and 10,000 test images, each of size $(28, 28)$. However...\n",
        "\n",
        "> ‚ùóÔ∏è  **`Convolutional Neural Network models need to be fed with images whose last dimension is the number of channels`.**  \n",
        "\n",
        "> üßëüèª‚Äçüè´ The shape of tensors fed into ***ConvNets*** is the following: `(NUMBER_OF_IMAGES, HEIGHT, WIDTH, CHANNELS)`\n",
        "\n",
        "üïµüèªThis last dimension is clearly missing here. Can you guess the reason why?\n",
        "<br>\n",
        "<details>\n",
        "    <summary><i>Answer<i></summary>\n",
        "        \n",
        "* All these $60000$ $ (28 \\times 28) $ pictures are black-and-white $ \\implies $ Each pixel lives on a spectrum from full black (0) to full white (1).\n",
        "        \n",
        "    * Theoretically, you don't need to know the number of channels for a black-and-white picture since there is only 1 channel (the \"whiteness\" of \"blackness\" of a pixel). However, it is still mandatory for the model to have this number of channels explicitly stated.\n",
        "        \n",
        "    * In comparison, colored pictures need multiple channels:\n",
        "        - the RGB system with 3 channels (<b><span style=\"color:red\">Red</span> <span style=\"color:green\">Green</span> <span style=\"color:blue\">Blue</span></b>)\n",
        "        - the CYMK system  with 4 channels (<b><span style=\"color:cyan\">Cyan</span> <span style=\"color:magenta\">Magenta</span> <span style=\"color:yellow\">Yellow</span> <span style=\"color:black\">Black</span></b>)\n",
        "        \n",
        "        \n",
        "</details>        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eyvovf7t7cOo"
      },
      "source": [
        "‚ùì **Question: expanding dimensions** ‚ùì\n",
        "\n",
        "* Use the **`expand_dims`** to add one dimension at the end of the training data and test data.\n",
        "\n",
        "* Then, print the shapes of `X_train` and `X_test`. They should respectively be equal to $(60000, 28, 28, 1)$ and $(10000, 28, 28, 1)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SU8z5iAS7cOp"
      },
      "outputs": [],
      "source": [
        "# Reshape the X to explicitly add a single \"color\" channel\n",
        "from tensorflow.keras.backend import expand_dims\n",
        "\n",
        "#X_train = expand_dims(X_train, axis=3)\n",
        "#X_test = expand_dims(X_test, axis=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1efkcUtu7cOn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac76496b-44c1-49da-d47d-768c8fe9f619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDbUt57N7cOq"
      },
      "source": [
        "### (1.4) Target encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FVfmFph7cOq"
      },
      "source": [
        "One more thing to for a multiclass classification task in Deep Leaning:\n",
        "\n",
        "üëâ _\"one-hot-encode\" the categories*_\n",
        "\n",
        "‚ùì **Question: encoding the labels** ‚ùì \n",
        "\n",
        "* Use **`to_categorical`** to transform your labels. \n",
        "* Store the results into two variables that you can call **`y_train_cat`** and **`y_test_cat`**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OoQmgipM7cOr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a50289-ab65-446d-84a8-85441fff9b73"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train_cat = to_categorical(y_train, num_classes=10)\n",
        "y_test_cat = to_categorical(y_test, num_classes=10)\n",
        "y_train_cat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "dyGdDGve7cOs"
      },
      "outputs": [],
      "source": [
        "# Quick check that you correctly used to_categorical\n",
        "assert(y_train_cat.shape == (60000,10))\n",
        "assert(y_test_cat.shape == (10000,10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXYfmkwK7cOs"
      },
      "source": [
        "The data is now ready to be used. ‚úÖ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlETJIzv7cOs"
      },
      "source": [
        "## (2) The Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4-kBoKr7cOt"
      },
      "source": [
        "### (2.1) Architecture and compilation of a CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccDItF1H7cOt"
      },
      "source": [
        "\n",
        "‚ùì **Question: CNN Architecture and compilation** ‚ùì\n",
        "\n",
        "Now, let's build a <u>Convolutional Neural Network</u> that has: \n",
        "\n",
        "\n",
        "- a `Conv2D` layer with 8 filters, each of size $(4, 4)$, an input shape suitable for your task, the `relu` activation function, and `padding='same'`\n",
        "- a `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
        "- a second `Conv2D` layer with 16 filters, each of size $(3, 3)$, and the `relu` activation function\n",
        "- a second `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
        "\n",
        "\n",
        "- a `Flatten` layer\n",
        "- a first `Dense` layer with 10 neurons and the `relu` activation function\n",
        "- a last (predictive) layer that is suited for your task\n",
        "\n",
        "In the function that initializes this model, do not forget to include the <u>compilation of the model</u>, which:\n",
        "* optimizes the `categorical_crossentropy` loss function,\n",
        "* with the `adam` optimizer, \n",
        "* and the `accuracy` as the metrics\n",
        "\n",
        "(*Note: you could add more classification metrics if you want but the dataset is well balanced!*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "OkBwZds37cOu"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "\n",
        "def initialize_model():\n",
        "\n",
        "    model = models.Sequential()\n",
        "\n",
        "    ### First Convolution & MaxPooling\n",
        "    model.add(layers.Conv2D(8, (4,4), input_shape=(28, 28, 1), padding='same', activation=\"relu\"))\n",
        "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
        "    \n",
        "    ### Second Convolution & MaxPooling\n",
        "    model.add(layers.Conv2D(16, (4,4), activation=\"relu\")) #padding='same', \n",
        "    model.add(layers.MaxPool2D(pool_size=(2,2))) \n",
        "    \n",
        "    ### Flattening\n",
        "    model.add(layers.Flatten())\n",
        "    \n",
        "    ### One Fully Connected layer - \"Fully Connected\" is equivalent to saying \"Dense\"\n",
        "    model.add(layers.Dense(10, activation='relu'))\n",
        "    \n",
        "    ### Last layer - Classification Layer with 10 outputs corresponding to 10 digits\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "    \n",
        "    ### Model compilation\n",
        "    model.compile(loss='categorical_crossentropy', \n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNFKlC8p7cOv"
      },
      "source": [
        "‚ùì **Question: number of trainable parameters in a convolutional layer** ‚ùì \n",
        "\n",
        "How many trainable parameters are there in your model?\n",
        "1. Compute them with ***model.summary( )*** first\n",
        "2. Recompute them manually to make sure you properly understood ***what influences the number of weights in a CNN***."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "tags": [
          "challengify"
        ],
        "id": "DNsbDeOR7cOv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "270882f8-a4f3-402f-f922-fa50ab6831a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 8)         136       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 8)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 16)        2064      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 16)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 400)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                4010      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                110       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,320\n",
            "Trainable params: 6,320\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = initialize_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ7PvoIe7cOw"
      },
      "source": [
        "### (2.2) Training a CNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv1 = 8*1*(4*4) + 8 # 8 filters x 1 channel x 4x4 kernel + output (8 filters)\n",
        "conv2 = 16*1*(4*4) + 16 # 16 filters x 1 channel x 4x4 kernel + output (8 filters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XlpNDOVU61T",
        "outputId": "7309c8ef-ffae-45bf-dfd4-4d830b0dedac"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "138"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL9Hqihl7cOw"
      },
      "source": [
        "‚ùì **Question: training a CNN** ‚ùì \n",
        "\n",
        "Initialize your model and fit it on the train data. \n",
        "- Do not forget to use a **Validation Set/Split** and an **Early Stopping criterion**. \n",
        "- Limit yourself to 5 epochs max in this challenge, just to save some precious time for the more advanced challenges!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7owJu557cOw"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "pass  # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu-pYEvq7cOx"
      },
      "source": [
        "‚ùì **Question: How many iterations does the CNN perform per epoch** ‚ùì\n",
        "\n",
        "_Note: it has nothing to do with the fact that this is a CNN. This is related to the concept of forward/backward propagation already covered during the previous lecture on optimizers, fitting, and losses üòâ_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "challengify"
        ],
        "id": "2KrIIrGc7cOy"
      },
      "source": [
        "> YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmr32KCu7cOy"
      },
      "source": [
        "<details>\n",
        "    <summary><i>Answer</i></summary>\n",
        "\n",
        "With `verbose = 1` when fitting your model, you have access to crucial information about your training procedure.\n",
        "    \n",
        "Remember that we've just trained our CNN model on $60000$ training images\n",
        "\n",
        "If the chosen batch size is 32: \n",
        "\n",
        "* For each epoch, we have $ \\large \\lceil \\frac{60000}{32} \\rceil = 1875$ minibatches <br/>\n",
        "* The _validation_split_ is equal to $0.3$ - which means that within one single epoch, there are:\n",
        "    * $ \\lceil 1875 \\times (1 - 0.3) \\rceil = \\lceil 1312.5 \\rceil = 1313$ batches are used to compute the `train_loss` \n",
        "    * $ 1875 - 1312 = 562 $ batches are used to compute the `val_loss`\n",
        "    * **The parameters are updated 1313 times per epoch** as there are 1313 forward/backward propagations per epoch !!!\n",
        "\n",
        "\n",
        "üëâ With so many updates of the weights within one epoch, you can understand why this CNN model converges even with a limited number of epochs.\n",
        "\n",
        "</details>    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuU5RYBd7cOz"
      },
      "source": [
        "### (2.3) Evaluating its performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlTwWe4a7cOz"
      },
      "source": [
        "‚ùì **Question: Evaluating your CNN** ‚ùì \n",
        "\n",
        "What is your **`accuracy on the test set?`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "challengify"
        ],
        "id": "eICoAgpk7cO0"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERilwGqq7cO0"
      },
      "source": [
        "üéâ You should already be impressed by your CNN skills! Reaching over 95% accuracy!\n",
        "\n",
        "üî• You solved what was a very hard problem 30 years ago with your own CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veMN71787cO1"
      },
      "source": [
        "üèÅ **Congratulations!**\n",
        "\n",
        "üíæ Don't forget to `git add/commit/push` your notebook...\n",
        "\n",
        "üöÄ ... and move on to the next challenge!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}